{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "f7f7e939",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F \n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "from uuid import uuid4, UUID\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "\n",
    "import syft as sy\n",
    "\n",
    "from util import Client, Server\n",
    "\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f1cc13",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "9fb8a169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerticalDataset(Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, ids, data, targets, *args, **kwargs):\n",
    "        'Initialization'\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ids = ids\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        uuid = self.ids[index]\n",
    "        if self.data is not None:\n",
    "            X = self.data[index]\n",
    "        else:\n",
    "            X = None\n",
    "        if self.targets is not None:\n",
    "            y = self.targets[index]\n",
    "        else:\n",
    "            y = None\n",
    "        return (*filter(lambda x: x is not None, (uuid, X, y)),)\n",
    "    \n",
    "    def get_ids(self) -> List[str]:\n",
    "        \"\"\"Return a list of the ids of this dataset.\"\"\"\n",
    "        return [str(_) for _ in self.ids]\n",
    "    \n",
    "    def sort_by_ids(self):\n",
    "        \"\"\"Sort the dataset by IDs in ascending order\"\"\"\n",
    "        ids = self.get_ids()\n",
    "        sorted_idxs = np.argsort(ids)\n",
    "\n",
    "        if self.data is not None:\n",
    "            self.data = self.data[sorted_idxs] \n",
    "\n",
    "        if self.targets is not None:\n",
    "            self.targets = self.targets[sorted_idxs]\n",
    "\n",
    "        self.ids = self.ids[sorted_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "71736da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ids(cls):\n",
    "    \"\"\"Decorator to add unique IDs to a dataset\n",
    "\n",
    "    Args:\n",
    "        cls (torch.utils.data.Dataset) : dataset to generate IDs for\n",
    "\n",
    "    Returns:\n",
    "        VerticalDataset : A class which wraps cls to add unique IDs as an attribute,\n",
    "            and returns data, target, id when __getitem__ is called\n",
    "    \"\"\"\n",
    "    class VerticalDataset(cls):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "\n",
    "            self.ids = np.array([uuid4() for _ in range(len(self))])\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            uuid = self.ids[index]\n",
    "\n",
    "            if self.data is not None:\n",
    "                X = self.data[index]\n",
    "            else:\n",
    "                X = None\n",
    "\n",
    "            if self.targets is not None:\n",
    "                y = self.targets[index]\n",
    "            else:\n",
    "                y = None\n",
    "            return (*filter(lambda x: x is not None, (uuid, X, y)),)\n",
    "\n",
    "        def __len__(self):\n",
    "            if self.data is not None:\n",
    "                return self.data.size(0)\n",
    "            else:\n",
    "                return len(self.targets)\n",
    "\n",
    "        def get_ids(self) -> List[str]:\n",
    "            \"\"\"Return a list of the ids of this dataset.\"\"\"\n",
    "            return [str(id_) for id_ in self.ids]\n",
    "\n",
    "        def sort_by_ids(self):\n",
    "            \"\"\"\n",
    "            Sort the dataset by IDs in ascending order\n",
    "            \"\"\"\n",
    "            ids = self.get_ids()\n",
    "            sorted_idxs = np.argsort(ids)\n",
    "\n",
    "            if self.data is not None:\n",
    "                self.data = self.data[sorted_idxs]\n",
    "\n",
    "            if self.targets is not None:\n",
    "                self.targets = self.targets[sorted_idxs]\n",
    "\n",
    "            self.ids = self.ids[sorted_idxs]\n",
    "\n",
    "    return VerticalDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "1fdd8748",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset(\n",
    "    dataset: Dataset,\n",
    "    keep_order: bool = False,\n",
    ") -> Tuple[Dataset, Dataset]:\n",
    "    'Vertically partition a torch dataset in two'\n",
    "    partition1 = copy.deepcopy(dataset)\n",
    "    partition2 = copy.deepcopy(dataset)\n",
    "    \n",
    "    # p1 has all features, p2 has all targets\n",
    "    partition1.targets = None\n",
    "    partition2.data = None\n",
    "    \n",
    "    # disorder indexing\n",
    "    idxs1 = np.arange(len(partition1)) \n",
    "    idxs2 = np.arange(len(partition2))\n",
    "    \n",
    "    if not keep_order:\n",
    "        np.random.shuffle(idxs1)\n",
    "        np.random.shuffle(idxs2)\n",
    "        \n",
    "    partition1.data = partition1.data[idxs1]\n",
    "    partition1.ids = partition1.ids[idxs1]\n",
    "\n",
    "    partition2.targets = partition2.targets[idxs2]\n",
    "    partition2.ids = partition2.ids[idxs2]\n",
    "    \n",
    "    return partition1, partition2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "1c386a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_collate_fn(batch: Tuple) -> List:\n",
    "    \"\"\"Collate data, targets and IDs  into batches\n",
    "\n",
    "    This custom function is necessary as default collate\n",
    "    functions cannot handle UUID objects.\n",
    "\n",
    "    Args:\n",
    "        batch (tuple of (data, target, id) tuples) : tuple of data returns from each index call\n",
    "            to the dataset in a batch. To be turned into batched data\n",
    "\n",
    "    Returns:\n",
    "        list : List of batched data objects:\n",
    "            data (torch.Tensor), targets (torch.Tensor), IDs (tuple of strings)\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for samples in zip(*batch):\n",
    "        if isinstance(samples[0], UUID):\n",
    "            # Turn into a tuple of strings\n",
    "            samples = (*map(str, samples),)\n",
    "\n",
    "        # Batch data\n",
    "        results.append(default_collate(samples))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "7a5453f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinglePartitionDataLoader(DataLoader):\n",
    "    \"\"\"DataLoader for a single vertically-partitioned dataset\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.collate_fn = id_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425ceb1d",
   "metadata": {},
   "source": [
    "## Partitioned Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "a611860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerticalDataLoader:\n",
    "    def __init__(self, dataset, *args, **kwargs):\n",
    "        # Split datasets\n",
    "        self.partition1, self.partition2 = partition_dataset(dataset)\n",
    "        \n",
    "        assert self.partition1.targets is None\n",
    "        assert self.partition2.data is None\n",
    "        \n",
    "        self.dataloader1 = SinglePartitionDataLoader(self.partition1, *args, **kwargs)\n",
    "        self.dataloader2 = SinglePartitionDataLoader(self.partition2, *args, **kwargs)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.dataloader1) + len(self.dataloader2)) // 2\n",
    "    \n",
    "    def __iter__(self):\n",
    "        return zip(self.dataloader1, self.dataloader2)\n",
    "    \n",
    "    def drop_non_intersection(self, intersection: List[int]):\n",
    "        \"\"\"Remove elements and ids in the datasets that are not in the intersection.\"\"\"\n",
    "        self.dataloader1.dataset.data = self.dataloader1.dataset.data[intersection]\n",
    "        self.dataloader1.dataset.ids = self.dataloader1.dataset.ids[intersection]\n",
    "\n",
    "        self.dataloader2.dataset.targets = self.dataloader2.dataset.targets[intersection]\n",
    "        self.dataloader2.dataset.ids = self.dataloader2.dataset.ids[intersection]\n",
    "        \n",
    "    def sort_by_ids(self) -> None:\n",
    "        \"\"\"Sort each dataset by ids\"\"\"\n",
    "        self.dataloader1.dataset.sort_by_ids()\n",
    "        self.dataloader2.dataset.sort_by_ids()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e5ce5b",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "371742ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Dataset() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/00/khw_9d2x7wq593j9szf1g28m0000gn/T/ipykernel_23584/3829341021.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# data = VerticalDataset(ids, features, labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVerticalDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Dataset() takes no arguments"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "params = {'batch_size': 1,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "\n",
    "max_epochs = 100\n",
    "\n",
    "# Dataset\n",
    "ids = np.array([uuid4() for i in range(10)])\n",
    "features = torch.randn((10, 30))\n",
    "labels = torch.randint(0, 2, (10,))\n",
    "\n",
    "# Generator\n",
    "data = VerticalDataset(ids, features, labels)\n",
    "# data = add_ids(Dataset(features, labels))\n",
    "dataloader = VerticalDataLoader(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7f97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vars(dataloader.dataloader1.dataset))\n",
    "\n",
    "print(vars(dataloader.dataloader2.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90604ce",
   "metadata": {},
   "source": [
    "## Implement PSI and order the datasets accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8311e905",
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataloader.dataloader1.dataset.ids[:].all() != dataloader.dataloader2.dataset.ids[:].all():\n",
    "    print(\"Partitioned data is disordered\")\n",
    "    \n",
    "# Compute private set intersection\n",
    "client_items = dataloader.dataloader1.dataset.get_ids()\n",
    "server_items = dataloader.dataloader2.dataset.get_ids()\n",
    "\n",
    "client = Client(client_items)\n",
    "server = Server(server_items)\n",
    "\n",
    "setup, response = server.process_request(client.request, len(client_items))\n",
    "intersection = client.compute_intersection(setup, response)\n",
    "\n",
    "# Order data\n",
    "dataloader.drop_non_intersection(intersection)\n",
    "dataloader.sort_by_ids()\n",
    "\n",
    "if dataloader.dataloader1.dataset.ids[:].all() == dataloader.dataloader2.dataset.ids[:].all():\n",
    "    print(\"Partitioned data is aligned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3740407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d50fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.seed = 0\n",
    "        self.input_size = 30 # 30 dimensions\n",
    "        self.hidden_sizes = [64, 16, 4] # can be altered\n",
    "        self.output_size = 2 # 0 or 1\n",
    "    \n",
    "args = Parser()\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038aefca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net():\n",
    "    def __init__(self, models, optimizers):\n",
    "        self.models = models\n",
    "        self.optimizers = optimizers\n",
    "        \n",
    "        self.data = []\n",
    "        self.remote_tensors = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        data = []\n",
    "        remote_tensors = []\n",
    "\n",
    "#         data.append(self.models[0](x))\n",
    "#         print(data)\n",
    "        return x\n",
    "    \n",
    "    def backward(self):\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def zero_grads(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f1fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitNN:\n",
    "    def __init__(self, models, optimizers):\n",
    "        self.models = models\n",
    "        self.optimizers = optimizers\n",
    "\n",
    "        self.data = []\n",
    "        self.remote_tensors = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        data = []\n",
    "        remote_tensors = []\n",
    "\n",
    "        data.append(self.models[0](x))\n",
    "\n",
    "        if data[-1].location == self.models[1].location:\n",
    "            remote_tensors.append(data[-1].detach().requires_grad_())\n",
    "        else:\n",
    "            remote_tensors.append(\n",
    "                data[-1].detach().move(self.models[1].location).requires_grad_()\n",
    "            )\n",
    "\n",
    "        i = 1\n",
    "        while i < (len(models) - 1):\n",
    "            data.append(self.models[i](remote_tensors[-1]))\n",
    "\n",
    "            if data[-1].location == self.models[i + 1].location:\n",
    "                remote_tensors.append(data[-1].detach().requires_grad_())\n",
    "            else:\n",
    "                remote_tensors.append(\n",
    "                    data[-1].detach().move(self.models[i + 1].location).requires_grad_()\n",
    "                )\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        data.append(self.models[i](remote_tensors[-1]))\n",
    "\n",
    "        self.data = data\n",
    "        self.remote_tensors = remote_tensors\n",
    "\n",
    "        return data[-1]\n",
    "    \n",
    "    def backward(self):\n",
    "        for i in range(len(models) - 2, -1, -1):\n",
    "            if self.remote_tensors[i].location == self.data[i].location:\n",
    "                grads = self.remote_tensors[i].grad.copy()\n",
    "            else:\n",
    "                grads = self.remote_tensors[i].grad.copy().move(self.data[i].location)\n",
    "    \n",
    "            self.data[i].backward(grads)\n",
    "\n",
    "    def zero_grads(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e27737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create workers\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "workers = [alice, bob]\n",
    "\n",
    "# create models\n",
    "models = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(args.input_size, args.hidden_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(args.hidden_sizes[0], args.hidden_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(args.hidden_sizes[1], args.hidden_sizes[2]),\n",
    "        nn.ReLU(),\n",
    "    ),\n",
    "    nn.Sequential(nn.Linear(args.hidden_sizes[2], args.output_size), nn.LogSoftmax(dim=1)),\n",
    "]\n",
    "\n",
    "# init optimizers\n",
    "optimizers = [optim.SGD(model.parameters(), lr=args.lr,) for model in models]\n",
    "\n",
    "# send models to each working node\n",
    "for model, worker in zip(models, workers):\n",
    "    model.send(worker)\n",
    "    \n",
    "# init splitNN\n",
    "splitNN = Net(models, optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b016db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features, labels, network):\n",
    "    \n",
    "    #1) Zero our grads\n",
    "    network.zero_grads()\n",
    "    \n",
    "    #2) Make a prediction\n",
    "    pred = network.forward(features)\n",
    "    \n",
    "    #3) Figure out how much we missed by\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(pred, labels)\n",
    "    \n",
    "    #4) Backprop the loss on the end layer\n",
    "    loss.backward()\n",
    "    \n",
    "    #5) Feed Gradients backward through the network\n",
    "    network.backward()\n",
    "    \n",
    "    #6) Change the weights\n",
    "    network.step()\n",
    "    \n",
    "    return loss, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c267ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    running_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    for (ids1, features), (ids2, labels) in dataloader:\n",
    "        # format data\n",
    "        features = features.send(models[0].location)\n",
    "        features = features.view(features.shape[0], -1)\n",
    "        labels = labels.send(models[-1].location)\n",
    "#     #         labels = labels.view(len(labels), 1)\n",
    "        print(features, labels)\n",
    "\n",
    "        # training\n",
    "        loss, preds = train(features, labels, splitNN)\n",
    "\n",
    "        # Collect statistics\n",
    "        running_loss += loss.get()\n",
    "        correct_preds += preds.max(1)[1].eq(labels).sum().get().item()\n",
    "        total_preds += preds.get().size(0)\n",
    "\n",
    "    print(f\"Epoch {i} - Training loss: {running_loss/len(dataloader):.3f} - Accuracy: {100*correct_preds/total_preds:.3f}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d20a92e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21301bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyvertical-dev] *",
   "language": "python",
   "name": "conda-env-pyvertical-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
