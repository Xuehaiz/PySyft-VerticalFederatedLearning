{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "077297f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F \n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "\n",
    "from typing import List, Tuple\n",
    "import random\n",
    "from uuid import uuid4, UUID\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import timeit\n",
    "\n",
    "import syft as sy\n",
    "\n",
    "from dataloader import VerticalDataLoader\n",
    "from src.psi.util import Client, Server\n",
    "# from util import Client, Server\n",
    "\n",
    "hook = sy.TorchHook(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9f62ce61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fa85a11b0d0>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Parser:\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.01\n",
    "        self.seed = 0\n",
    "        self.input_size = 30 # 30 dimensions\n",
    "        self.hidden_sizes = [64, 16, 4] # can be altered\n",
    "        self.output_size = 2 # 0 or 1\n",
    "        self.batch_size = 128\n",
    "    \n",
    "args = Parser()\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62883418",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f9f56532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerticalDataset(Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, ids, data, targets, *args, **kwargs):\n",
    "        'Initialization'\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.ids = ids\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Load data and get label\n",
    "        uuid = self.ids[index]\n",
    "        if self.data is not None:\n",
    "            X = self.data[index]\n",
    "        else:\n",
    "            X = None\n",
    "        if self.targets is not None:\n",
    "            y = self.targets[index]\n",
    "        else:\n",
    "            y = None\n",
    "        return (*filter(lambda x: x is not None, (uuid, X, y)),)\n",
    "    \n",
    "    def get_ids(self) -> List[str]:\n",
    "        \"\"\"Return a list of the ids of this dataset.\"\"\"\n",
    "        return [str(_) for _ in self.ids]\n",
    "    \n",
    "    def sort_by_ids(self):\n",
    "        \"\"\"Sort the dataset by IDs in ascending order\"\"\"\n",
    "        ids = self.get_ids()\n",
    "        sorted_idxs = np.argsort(ids)\n",
    "\n",
    "        if self.data is not None:\n",
    "            self.data = self.data[sorted_idxs] \n",
    "\n",
    "        if self.targets is not None:\n",
    "            self.targets = self.targets[sorted_idxs]\n",
    "\n",
    "        self.ids = self.ids[sorted_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e027b55",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "06f8785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "ids = np.array([uuid4() for i in range(10 ** 4)])\n",
    "features = torch.randn((10 ** 4, 30))\n",
    "# features = torch.from_numpy(np.random.randn(10, 30).astype(np.float32))\n",
    "labels = torch.randint(0, 2, (10 ** 4,))\n",
    "\n",
    "# Generator\n",
    "data = VerticalDataset(ids, features, labels)\n",
    "# data = add_ids(Dataset(features, labels))\n",
    "dataloader = VerticalDataLoader(data, batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fede78",
   "metadata": {},
   "source": [
    "## Implement PSI and order the datasets accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "2f6e9d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitioned data is disordered\n",
      "Partitioned data is aligned\n"
     ]
    }
   ],
   "source": [
    "if dataloader.dataloader1.dataset.ids[:].all() != dataloader.dataloader2.dataset.ids[:].all():\n",
    "    print(\"Partitioned data is disordered\")\n",
    "    \n",
    "# Compute private set intersection\n",
    "client_items = dataloader.dataloader1.dataset.get_ids()\n",
    "server_items = dataloader.dataloader2.dataset.get_ids()\n",
    "\n",
    "client = Client(client_items)\n",
    "server = Server(server_items)\n",
    "\n",
    "setup, response = server.process_request(client.request, len(client_items))\n",
    "intersection = client.compute_intersection(setup, response)\n",
    "\n",
    "# Order data\n",
    "dataloader.drop_non_intersecting(intersection)\n",
    "dataloader.sort_by_ids()\n",
    "\n",
    "if dataloader.dataloader1.dataset.ids[:].all() == dataloader.dataloader2.dataset.ids[:].all():\n",
    "    print(\"Partitioned data is aligned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a8a6c0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitNN:\n",
    "    def __init__(self, models, optimizers):\n",
    "        self.models = models\n",
    "        self.optimizers = optimizers\n",
    "\n",
    "        self.data = []\n",
    "        self.remote_tensors = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        data = []\n",
    "        remote_tensors = []\n",
    "\n",
    "        data.append(self.models[0](x))\n",
    "\n",
    "        if data[-1].location == self.models[1].location:\n",
    "            remote_tensors.append(data[-1].detach().requires_grad_())\n",
    "        else:\n",
    "            remote_tensors.append(data[-1].detach().move(self.models[1].location).requires_grad_())\n",
    "\n",
    "        i = 1\n",
    "        while i < (len(models) - 1):\n",
    "            data.append(self.models[i](remote_tensors[-1]))\n",
    "\n",
    "            if data[-1].location == self.models[i + 1].location:\n",
    "                remote_tensors.append(data[-1].detach().requires_grad_())\n",
    "            else:\n",
    "                remote_tensors.append(\n",
    "                    data[-1].detach().move(self.models[i + 1].location).requires_grad_()\n",
    "                )\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        data.append(self.models[i](remote_tensors[-1]))\n",
    "\n",
    "        self.data = data\n",
    "        self.remote_tensors = remote_tensors\n",
    "\n",
    "        return data[-1]\n",
    "    \n",
    "    def backward(self):\n",
    "        for i in range(len(models) - 2, -1, -1):\n",
    "            if self.remote_tensors[i].location == self.data[i].location:\n",
    "                grads = self.remote_tensors[i].grad.copy()\n",
    "            else:\n",
    "                grads = self.remote_tensors[i].grad.copy().move(self.data[i].location)\n",
    "    \n",
    "            self.data[i].backward(grads)\n",
    "\n",
    "    def zero_grads(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.zero_grad()\n",
    "\n",
    "    def step(self):\n",
    "        for opt in self.optimizers:\n",
    "            opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d5619616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create workers\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "workers = [alice, bob]\n",
    "\n",
    "# create models\n",
    "models = [\n",
    "    nn.Sequential(\n",
    "        nn.Linear(args.input_size, args.hidden_sizes[0]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(args.hidden_sizes[0], args.hidden_sizes[1]),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(args.hidden_sizes[1], args.hidden_sizes[2]),\n",
    "        nn.ReLU(),\n",
    "    ),\n",
    "    nn.Sequential(nn.Linear(args.hidden_sizes[2], args.output_size), nn.LogSoftmax(dim=1)),\n",
    "]\n",
    "\n",
    "# init optimizers\n",
    "optimizers = [optim.SGD(model.parameters(), lr=args.lr,) for model in models]\n",
    "\n",
    "# send models to each working node\n",
    "for model, worker in zip(models, workers):\n",
    "    model.send(worker)\n",
    "    \n",
    "# init splitNN\n",
    "splitNN = SplitNN(models, optimizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "42b6eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(features, labels, network):\n",
    "    \n",
    "    #1) Zero our grads\n",
    "    network.zero_grads()\n",
    "    \n",
    "    #2) Make a prediction\n",
    "    pred = network.forward(features)\n",
    "    \n",
    "    #3) Figure out how much we missed by\n",
    "    criterion = nn.MSELoss()\n",
    "    loss = criterion(pred, labels.float())\n",
    "    \n",
    "    #4) Backprop the loss on the end layer\n",
    "    loss.backward()\n",
    "    \n",
    "    #5) Feed Gradients backward through the network\n",
    "    network.backward()\n",
    "    \n",
    "    #6) Change the weights\n",
    "    network.step()\n",
    "    \n",
    "    return loss, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d9a5d77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timeit.default_timer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "37b79177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 1.696 - Accuracy: 6482.800\n",
      "Epoch 1 - Training loss: 1.695 - Accuracy: 6379.660\n",
      "Epoch 2 - Training loss: 1.695 - Accuracy: 6361.560\n",
      "Epoch 3 - Training loss: 1.695 - Accuracy: 6356.540\n",
      "Epoch 4 - Training loss: 1.695 - Accuracy: 6353.760\n",
      "Epoch 5 - Training loss: 1.695 - Accuracy: 6350.160\n",
      "Epoch 6 - Training loss: 1.695 - Accuracy: 6350.700\n",
      "Epoch 7 - Training loss: 1.695 - Accuracy: 6348.840\n",
      "Epoch 8 - Training loss: 1.695 - Accuracy: 6348.800\n",
      "Epoch 9 - Training loss: 1.695 - Accuracy: 6346.900\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    running_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    for (ids1, features), (ids2, labels) in dataloader:\n",
    "        # format data\n",
    "        features = features.send(models[0].location)\n",
    "        labels = labels.send(models[-1].location)\n",
    "        labels = labels.view(-1, 1)\n",
    "        \n",
    "        # training\n",
    "        loss, preds = train(features, labels, splitNN)\n",
    "\n",
    "        # Collect statistics\n",
    "        running_loss += loss.get()\n",
    "        correct_preds += preds.max(1)[1].eq(labels).sum().get().item()\n",
    "        total_preds += preds.get().size(0)\n",
    "\n",
    "    print(f\"Epoch {epoch} - Training loss: {running_loss/len(dataloader):.3f} - Accuracy: {100*correct_preds/total_preds:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c5e69d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time:  21.634778955\n"
     ]
    }
   ],
   "source": [
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyvertical-dev] *",
   "language": "python",
   "name": "conda-env-pyvertical-dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
